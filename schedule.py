import json
import logging
import pickle
import time
import sys
import os
sys.path.append(os.getcwd() + '/src')  # noqa: E402 hack: could use PYTHONPATH
import math
from collections import OrderedDict
import numpy as np
import PyExpUtils.runner.Slurm as Slurm
import PyExpUtils.runner.parallel as Parallel
from PyExpUtils.utils.generator import group
from utils.hypers import total
import click
from socket import gethostname

fewest_gpu = {
    "cedar": 4,
    "graham": 2,
}

cwd = os.getcwd()

def using_gpu(gpu):
    return (isinstance(gpu, int) and gpu > 0) or isinstance(gpu, str)

def get_fewest_gpus():
    host = gethostname()
    if "cedar" in host:
        return fewest_gpu["cedar"]
    elif "gra" in host:
        return fewest_gpu["graham"]


def getJobScript(parallel, num_gpu):
    # The contents of the string below will be the bash script that is
    # scheduled on Compute Canada. Change the script accordingly
    # (e.g. add the necessary `module load X` commands etc.)
    if isinstance(num_gpu, int) and num_gpu <= 0:
        gpus = ""
    else:
        if num_gpu == "fewest":
            num_gpu = get_fewest_gpus()
        elif not isinstance(num_gpu, int):
            raise ValueError(f"unknown gpu configuration {num_gpu}")

        gpus = f"#SBATCH --gres=gpu:{num_gpu}"

    return f"""#!/bin/bash
{gpus}

cd {cwd}
module load python/3.9
module load cuda
module load cudnn/8.0.3

source ~/py3_9/bin/activate

export MPLBACKEND=TKAgg
export OMP_NUM_THREADS=1
{parallel}
    """


def printProgress(size, it):
    for i, _ in enumerate(it):
        print(f'{i + 1}/{size}', end='\r')
        if i - 1 == size:
            print()
        yield _


def exp_dir_name(dir_prefix, agent_config, env_config):
    """ exp_dir_name gets the directory at which the data generated by
    the experiment to run will be saved in """
    agent = agent_config["agent_name"]
    param = agent_config["parameters"]["policy_type"][0]

    env = env_config["env_name"]
    return os.path.join(dir_prefix, f"{env}_{agent}_{param}")


def estimateUsage(indices, groupSize, cores, hours):
    jobs = math.ceil(len(indices) / groupSize)

    total_cores = jobs * cores
    core_hours = total_cores * hours

    core_years = core_hours / (24 * 365)
    allocation = 724

    return core_years, 100 * core_years / allocation


def gatherMissing(
    dir_prefix, agent_paths, env_path, runs, groupSize, cores, total_hours,
    debug,
):
    out = {}

    approximate_cost = np.zeros(2)

    for path in agent_paths:
        indices = set()
        with open(path, 'r') as agent_dat:
            agent_json = json.load(agent_dat, object_pairs_hook=OrderedDict)

        with open(env_path, 'r') as env_dat:
            env_json = json.load(env_dat, object_pairs_hook=OrderedDict)

        # Find the number of permutations
        numPerms = total(agent_json["parameters"])
        if debug:
            print("\nTotal number of hyper settings:", numPerms)

        dir_ = os.path.join(
            "./results/",
            exp_dir_name(dir_prefix, agent_json, env_json),
        )
        if debug:
            print("Directory which should hold all data is:", dir_)

        # Check if the path exists, if not, then all indices need to be ran
        if not os.path.exists(dir_):
            if debug:
                print("Directory does not exists, all indices will be run")
            indices = range(numPerms * runs)

        elif not os.path.exists(os.path.join(dir_, "data.pkl")) and len(os.listdir(dir_)) == 0:
            if debug:
                print("Directory empty, all indices will be run")
            indices = range(numPerms * runs)

        # If individual data files exist in the directory, but not a main
        # data.pkl file, then get the indices to run from from the file names
        elif len(os.listdir(dir_)) != 0 and \
                not os.path.exists(os.path.join(dir_, "data.pkl")):
            env_name = env_json["env_name"]
            ag_name = agent_json["agent_name"]

            for i in range(numPerms * runs):
                perm = i % numPerms
                run = i // numPerms
                f_name = f"{env_name}_{ag_name}_data_{i}.pkl"
                if not os.path.exists(os.path.join(dir_, f_name)):
                    indices.add(i)

        # Otherwise, a data.pkl file exists with a number of results from
        # different indices, along with possibly individual results files for
        # some indices
        else:
            env_name = env_json["env_name"]
            ag_name = agent_json["agent_name"]
            with open(os.path.join(dir_, "data.pkl"), "rb") as infile:
                data = pickle.load(infile)

            for i in range(numPerms * runs):
                perm = i % numPerms
                run = i // numPerms
                f_name = f"{env_name}_{ag_name}_data_{i}.pkl"

                # Check if an individual data file exists for this hyper
                # setting
                file_exists = os.path.exists(os.path.join(dir_, f_name))
                if file_exists:
                    continue

                # Check if this hyper setting is in the main data file
                elif perm in data["experiment_data"]:
                    runs_completed = set()
                    for r in data["experiment_data"][perm]["runs"]:
                        runs_completed.add(r["run_number"])

                    if run in runs_completed:
                        continue
                else:
                    indices.add(i)

        indices = sorted(list(indices))
        out[path] = indices

        approximate_cost += estimateUsage(indices, groupSize, cores,
                                          total_hours)

        # figure out how many indices to expect
        size = numPerms * runs

        # log how many are missing
        print(path, f'{len(indices)} / {size}')
        print(indices)

    print(out.keys())
    return out, approximate_cost


# # Get command-line arguments
# slurm_path = sys.argv[1]
# executable = sys.argv[2]
# base_path = sys.argv[3]
# runs = int(sys.argv[4])
# env_path = sys.argv[5]
# dir_prefix = sys.argv[6]
# agent_paths = sys.argv[7:]


@click.command()
@click.argument("slurm_path")
@click.argument("executable")
@click.argument("base_path")
@click.argument("env_path")
@click.argument("dir_prefix")
@click.argument("agent_paths", nargs=-1)
@click.option(
    "-r", "--runs", required=True, type=int, help="number of runs to schedule",
)
@click.option(
    "-g", "--gpu", default=0, show_default=True, type=str,
    help="how many GPUs to " +
    "request for the job. If an integer > 0, then request that many GPUs. " +
    "If an integer <= 0, then use no GPUs. If 'fewest', then request the " +
    "fewest number of GPUs available on any node in the cluster",
)
@click.option(
    "-y", "--yes", is_flag=True, show_default=True,
    help="do not confirm before scheduling",
)
@click.option(
    "-d", "--debug", is_flag=True, show_default=True,
    help="show debugging messages",
)
@click.option(
    "--rerun", is_flag=True, show_default=True,
    help="Specify if we're reruning the experiment for the best " +
    "hyperparameters. If yes, we will use a different starting " +
    "random seed", default=False, type=bool, required=False,
)
def main(
    slurm_path,
    executable, base_path,
    env_path,
    dir_prefix,
    agent_paths,
    runs,
    gpu,
    yes,
    debug,
    rerun,
):
    # Scheduling logic
    slurm = Slurm.fromFile(slurm_path)

    # compute how many "tasks" to clump into each job
    groupSize = slurm.cores * slurm.sequential

    # compute how much time the jobs are going to take
    hours, minutes, seconds = slurm.time.split(':')
    total_hours = int(hours) + (int(minutes) / 60) + (int(seconds) / 3600)

    # gather missing and sum up cost
    missing, cost = gatherMissing(
        dir_prefix, agent_paths, env_path, runs, groupSize, slurm.cores,
        total_hours, debug,
    )

    if not yes:
        print()
        print(f"Expected to use {cost[0]:.2f} core years, which is " +
            f"{cost[1]:.4f}% of our annual allocation")
        input("Press Enter to confirm or ctrl+c to exit")

    for path in missing:
        # reload this because we do bad mutable things later on
        slurm = Slurm.fromFile(slurm_path)

        for g in group(missing[path], groupSize):
            tasks = list(g)

            # Convert gpus to int if possible
            if isinstance(gpu, str):
                gpu = int(gpu) if gpu.isdecimal() else gpu

            # build the executable string
            cuda_visible = ""
            if using_gpu(gpu):
                cuda_visible = "CUDA_VISIBLE_DEVICES=$(({%} - 1)); "

            # build the rerun flag
            rerun_flag = ""
            if rerun:
                rerun_flag = "--rerun "

            runner = f"'{cuda_visible}python " + \
                f"{executable} --env-json {env_path} " + \
                f"--agent-json {path} {rerun_flag}" + \
                f"--save-dir '\"{dir_prefix}\"' " + "--index {}'"

            # Ensure only to request the number of CPU cores necessary
            slurm.cores = min([slurm.cores, len(tasks)])

            # generate the gnu-parallel command for dispatching to many CPUs
            # across server nodes
            parallel = Slurm.buildParallel(
                runner, tasks, {"ntasks": slurm.cores},
            )
            print(parallel)

            # Ensure not to request more GPUs than CPUs
            if isinstance(gpu, int) and gpu > slurm.cores:
                gpu = slurm.cores
            elif gpu == "fewest" and get_fewest_gpus() > slurm.cores:
                gpu = slurm.cores
            elif isinstance(gpu, int) and (gpu > 0 and gpu < slurm.cores):
                logging.warning(
                    "Fewer GPUs than CPUs requested, are you sure this " +
                    "is what you wanted to do?"
                )
            script = getJobScript(parallel, gpu)

            # Uncomment for debugging the scheduler to see what bash script
            # would have been scheduled
            if debug:
                print(script)

            Slurm.schedule(script, slurm)

            # Prevent overburdening the slurm scheduler. Be a good citizen.
            time.sleep(2)

if __name__ == "__main__":
    main()
